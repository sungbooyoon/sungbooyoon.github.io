<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sungboo Yoon </title> <meta name="author" content="Sungboo Yoon"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/name_logo.png?bf96483cee2bf9c243f278286a18d827"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sungbooyoon.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Sungboo Yoon </h1> <p class="desc">Ph.D. Candidate in Architectural Engineering @ <a href="https://en.snu.ac.kr/" rel="external nofollow noopener" target="_blank">Seoul National University</a> (Advisor Prof. <a href="https://scholar.google.com/citations?user=PJfny7sAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Changbum R. Ahn</a>)</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile_square-480.webp 480w,/assets/img/profile_square-800.webp 800w,/assets/img/profile_square-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/profile_square.jpg?1dd366fab41ff111eedc94b12ab20dbd" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="profile_square.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="social"> <div class="contact-icons"> <a href="/assets/pdf/cv.pdf" title="CV" target="_blank"><i class="ai ai-cv"></i></a> <a href="https://github.com/sungbooyoon" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/sungboo-yoon-303662204" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://orcid.org/0000-0003-4997-5792" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=QFeaFbwAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://youtube.com/@ssungbooyoon" title="YouTube" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> <div class="contact-note"></div> </div> </div> <div class="clearfix"> <p>I am a third-year Ph.D. student in the <a href="https://architecture.snu.ac.kr/" rel="external nofollow noopener" target="_blank">Department of Architecture &amp; Architectural Engineering</a> at Seoul National University. I am fortunate to be advised by Professor Changbum R. Ahn, and I am a member of the <a href="https://cem.snu.ac.kr/" rel="external nofollow noopener" target="_blank">Construction Engineering &amp; Management Lab</a>. My research interests span the fields of Construction Robotics and Machine Learning. My research goal is to design innovative interfaces that enhance human-robot collaboration on construction sites, facilitating efficient and effective interaction.</p> <p>I previously completed both my bachelor’s and master’s degrees in Architectural Engineering at Seoul National University, where I conducted research on modular construction and human-robot interaction under the advisement of Professor <a href="https://scholar.google.com/citations?user=lkrSqmIAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Moonseo Park</a>.</p> <p>I am honored to be a recipient of the <a href="https://www.nrf.re.kr/biz/info/info/view?menu_no=378&amp;biz_no=416" rel="external nofollow noopener" target="_blank">Basic Science Research Program</a> (Ph.D. Fellowship) from the National Research Foundation of Korea (NRF).</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jul 21, 2025</th> <td> Our paper, “<a href="http://dx.doi.org/10.1016/j.aei.2025.103665" rel="external nofollow noopener" target="_blank">Learning viewpoint control from human-initiated transitions for teleoperation in construction</a>”, is now available online! </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 22, 2025</th> <td> Our paper, “<a href="https://www.sciencedirect.com/science/article/pii/S0926580525000937" rel="external nofollow noopener" target="_blank">Comparing dynamic viewpoint control techniques for teleoperated robotic welding in construction</a>”, is now available online! </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 19, 2024</th> <td> Check out <a href="https://webzine-eng.snu.ac.kr/web/snu_en/vol08/inno08_6.php" rel="external nofollow noopener" target="_blank">this article</a> on robotics research at Construction Engineering and Management Lab (SNUCEM). </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 01, 2024</th> <td> I was awarded the Basic Science Research Program (Ph.D. Fellowship) from the National Research Foundation of Korea (NRF). </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 15, 2024</th> <td> Finally migrated my personal website from google to github! <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div id="yoon2024laserdex" class="col-sm-10"> <div class="title">LaserDex: Improvising Spatial Tasks Using Deictic Gestures and Laser Pointing for Human–Robot Collaboration in Construction</div> <div class="author"> <em>Sungboo Yoon</em>, Moonseo Park, and Changbum R Ahn </div> <div class="periodical"> <em>Journal of Computing in Civil Engineering</em>. <strong>Editor’s Choice</strong> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1061/JCCEE5.CPENG-5715" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://ascelibrary.org/doi/10.1061/JCCEE5.CPENG-5715" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2405_JCCE.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In the context of the unstructured and fast-changing construction environment, the adaptability of robots to human improvisation is crucial. However, existing construction robots are limited in understanding spatial goals communicated spontaneously in the workplace and require substantial human-user training. Incorporating deictic gestures into human–robot interfaces holds promise for enhancing the intuitive operation of construction robots and for on-site collaboration. However, applying deictic gestures in precision-demanding construction tasks presents challenges because in a large-scale work environment, humans have limited accuracy in pointing at objects at a distance. This study introduces LaserDex, a novel interface for distant spatial tasking based on a global-to-local identification approach, in which the human first guides the robot toward the general area of the task using a deictic gesture and then indicates the precise area by dynamically using a laser pointer. Our user study with 11 participants demonstrated that LaserDex can achieve an intersection over union (IoU) of 0.830 when outlining a rectangular drywall opening (compared with an IoU of 0.514 for the baseline, a handheld controller), and an 11.4-mm distance error of the center of the estimated rectangle compared with the center of the targeted rectangle. The findings of this study underscore the potential of LaserDex to seamlessly combine intuitive human–robot interaction with precise robot operations.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yoon2023effects" class="col-sm-10"> <div class="title">Effects of Spatial Characteristics on the Human–Robot Communication Using Deictic Gesture in Construction</div> <div class="author"> <em>Sungboo Yoon</em>, YeSeul Kim, Moonseo Park, and Changbum R Ahn </div> <div class="periodical"> <em>Journal of Construction Engineering and Management</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1061/JCEMD4.COENG-12997" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://ascelibrary.org/doi/abs/10.1061/JCEMD4.COENG-12997" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2307_JCEM.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Construction robots are expected to frequently communicate in situ improvisations with human workers to adapt and change their workflow and methods. One way to achieve this is through deictic gestures that are one of the most effective forms of human–robot interaction (HRI) in delivering spatial information. Nevertheless, the limited coverage of deictic gestures in large-scale environments poses some challenges for both humans and robots in leveraging such techniques for HRI in construction. To identify the feasibility of deictic gestures in the construction domain and find applicable solutions for improving performance, this study aims to extend current knowledge on the performance in communicating positional information using deictic gestures by investigating the effects of spatial characteristics on spatial referencing, focusing on the target configuration, target distance, and relative position of human and robot. We observed that the recognition and estimation of deictic gestures were affected by the target plane, target position, and the target layout and that the robot performance was significantly reduced as the distance between the human and robot increased. The findings of this study demonstrate the challenges in spatial referencing within a large-scale environment and highlight the need for bidirectional communication in HRI.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Yoon2025Comparing" class="col-sm-10"> <div class="title">Comparing dynamic viewpoint control techniques for teleoperated robotic welding in construction</div> <div class="author"> <em>Sungboo Yoon</em>, Moonseo Park, and Changbum R. Ahn </div> <div class="periodical"> <em>Automation in Construction</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.autcon.2025.106053" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.sciencedirect.com/science/article/pii/S0926580525000937" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2504_AutoCon.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Dynamic viewpoints offer effective visual feedback in teleoperation for construction, where tasks often require precise manipulation during frequent viewpoint adjustments. However, the comparative performance of various dynamic viewpoint control techniques remains unclear. This paper investigates the impact of dynamic viewpoint control techniques on task performance and user experience during teleoperation in construction. A user study was conducted in a remote welding-at-height scenario with 20 participants, including experienced welders and university students, to compare five techniques: (1) coupled vision-motion, (2) decoupled vision-motion with hand or head motion-based control, and (3) hybrid vision-motion with manual or automatic switching. Results showed that decoupled vision-motion with head motion-based control outperformed other techniques in task efficiency and user preference. Hybrid vision-motion with manual switching was more effective than decoupled vision-motion in contexts involving occlusions, reducing physical demand and enhancing welding quality. Based on these findings, guidelines are proposed for viewpoint control in teleoperated construction robots.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yoon2025learning" class="col-sm-10"> <div class="title">Learning viewpoint control from human-initiated transitions for teleoperation in construction</div> <div class="author"> <em>Sungboo Yoon</em>, Moonseo Park, and Changbum R Ahn </div> <div class="periodical"> <em>Advanced Engineering Informatics</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.aei.2025.103665" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.sciencedirect.com/science/article/pii/S1474034625005580" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2511_ADVEI.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Visual perception is critical for teleoperation in construction, where optimal visibility directly impacts task performance. Hybrid viewpoint control systems enhance the flexibility of visual perception by adaptively coupling or decoupling the viewpoint from robot movements according to situational demands. However, determining the optimal timing for transitions between these perspectives remains a major challenge, as existing autonomous methods are not directly applicable to hybrid control for construction tasks. In this work, we propose a viewpoint control mode prediction model that autonomously manages transitions during teleoperation with hybrid control. Our learning scheme with a transition-guided weighting method leverages sporadic transition commands from human interactions with the teleoperation system as demonstration data for imitation learning. User evaluation in a virtual reality (VR) environment simulating construction welding tasks shows that our model outperforms the baselines, achieving an 11% improvement over the state-of-the-art behavioral cloning (BC) algorithm and a 19% improvement over the state-of-the-art weighted BC algorithm in replicating human transition behaviors. This work contributes novel insights into the design of visual perception systems for teleoperation in construction, enabling reliable, user-aligned viewpoint transitions.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Sungboo Yoon. Building 39 (Room 425), 1 Gwanak-ro, Gwanak-gu, Seoul, South Korea 08826 Last updated: September 15, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>