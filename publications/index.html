<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Sungboo Yoon </title> <meta name="author" content="Sungboo Yoon"> <meta name="description" content="These publications are also available on &lt;a href='https://scholar.google.com/citations?user=QFeaFbwAAAAJ'&gt;&lt;u&gt;Google Scholar&lt;/u&gt;&lt;/a&gt;."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/name_logo.png?bf96483cee2bf9c243f278286a18d827"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sungbooyoon.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Sungboo Yoon </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">These publications are also available on <a href="https://scholar.google.com/citations?user=QFeaFbwAAAAJ" rel="external nofollow noopener" target="_blank"><u>Google Scholar</u></a>.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div id="Yoon2025Comparing" class="col-sm-10"> <div class="title">Comparing dynamic viewpoint control techniques for teleoperated robotic welding in construction</div> <div class="author"> <em>Sungboo Yoon</em>, Moonseo Park, and Changbum R. Ahn </div> <div class="periodical"> <em>Automation in Construction</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.autcon.2025.106053" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.sciencedirect.com/science/article/pii/S0926580525000937" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2504_AutoCon.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Dynamic viewpoints offer effective visual feedback in teleoperation for construction, where tasks often require precise manipulation during frequent viewpoint adjustments. However, the comparative performance of various dynamic viewpoint control techniques remains unclear. This paper investigates the impact of dynamic viewpoint control techniques on task performance and user experience during teleoperation in construction. A user study was conducted in a remote welding-at-height scenario with 20 participants, including experienced welders and university students, to compare five techniques: (1) coupled vision-motion, (2) decoupled vision-motion with hand or head motion-based control, and (3) hybrid vision-motion with manual or automatic switching. Results showed that decoupled vision-motion with head motion-based control outperformed other techniques in task efficiency and user preference. Hybrid vision-motion with manual switching was more effective than decoupled vision-motion in contexts involving occlusions, reducing physical demand and enhancing welding quality. Based on these findings, guidelines are proposed for viewpoint control in teleoperated construction robots.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yoon2025learning" class="col-sm-10"> <div class="title">Learning viewpoint control from human-initiated transitions for teleoperation in construction</div> <div class="author"> <em>Sungboo Yoon</em>, Moonseo Park, and Changbum R Ahn </div> <div class="periodical"> <em>Advanced Engineering Informatics</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.aei.2025.103665" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.sciencedirect.com/science/article/pii/S1474034625005580" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2511_ADVEI.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Visual perception is critical for teleoperation in construction, where optimal visibility directly impacts task performance. Hybrid viewpoint control systems enhance the flexibility of visual perception by adaptively coupling or decoupling the viewpoint from robot movements according to situational demands. However, determining the optimal timing for transitions between these perspectives remains a major challenge, as existing autonomous methods are not directly applicable to hybrid control for construction tasks. In this work, we propose a viewpoint control mode prediction model that autonomously manages transitions during teleoperation with hybrid control. Our learning scheme with a transition-guided weighting method leverages sporadic transition commands from human interactions with the teleoperation system as demonstration data for imitation learning. User evaluation in a virtual reality (VR) environment simulating construction welding tasks shows that our model outperforms the baselines, achieving an 11% improvement over the state-of-the-art behavioral cloning (BC) algorithm and a 19% improvement over the state-of-the-art weighted BC algorithm in replicating human transition behaviors. This work contributes novel insights into the design of visual perception systems for teleoperation in construction, enabling reliable, user-aligned viewpoint transitions.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div id="yoon2024laserdex" class="col-sm-10"> <div class="title">LaserDex: Improvising Spatial Tasks Using Deictic Gestures and Laser Pointing for Human–Robot Collaboration in Construction</div> <div class="author"> <em>Sungboo Yoon</em>, Moonseo Park, and Changbum R Ahn </div> <div class="periodical"> <em>Journal of Computing in Civil Engineering</em>. <strong>Editor’s Choice</strong> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1061/JCCEE5.CPENG-5715" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://ascelibrary.org/doi/10.1061/JCCEE5.CPENG-5715" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2405_JCCE.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In the context of the unstructured and fast-changing construction environment, the adaptability of robots to human improvisation is crucial. However, existing construction robots are limited in understanding spatial goals communicated spontaneously in the workplace and require substantial human-user training. Incorporating deictic gestures into human–robot interfaces holds promise for enhancing the intuitive operation of construction robots and for on-site collaboration. However, applying deictic gestures in precision-demanding construction tasks presents challenges because in a large-scale work environment, humans have limited accuracy in pointing at objects at a distance. This study introduces LaserDex, a novel interface for distant spatial tasking based on a global-to-local identification approach, in which the human first guides the robot toward the general area of the task using a deictic gesture and then indicates the precise area by dynamically using a laser pointer. Our user study with 11 participants demonstrated that LaserDex can achieve an intersection over union (IoU) of 0.830 when outlining a rectangular drywall opening (compared with an IoU of 0.514 for the baseline, a handheld controller), and an 11.4-mm distance error of the center of the estimated rectangle compared with the center of the targeted rectangle. The findings of this study underscore the potential of LaserDex to seamlessly combine intuitive human–robot interaction with precise robot operations.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yoon2024deictic" class="col-sm-10"> <div class="title">A Deictic Gesture-Based Human-Robot Interface for In Situ Task Specification in Construction</div> <div class="author"> <em>Sungboo Yoon</em>, Jinsik Park, Moonseo Park, and Changbum R Ahn </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1061/9780784485224.054" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://ascelibrary.org/doi/abs/10.1061/9780784485224.054" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2401_i3CE.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p> Despite the potential of robotic systems for automating the construction industry, the role of human operators remains essential for the success of these systems in complex and dynamic environments. However, current human-robot interfaces are often limited to low-level interactions that require constant micromanaging of robot movements. To address this limitation, this study proposes a deictic gesture-based interface that enables high-level task specification for construction robots. To evaluate the user experience and task performance of the proposed interface, we conducted a laboratory experiment with six human subjects who interacted with the robot to make openings in drywall panels. The results show that the proposed interface significantly reduced mental demand and effort levels among the participants compared to the conventional joystick interface. Moreover, task performance using the proposed interface was comparable in accuracy and efficiency to that achieved with the joystick interface. These findings highlight the potential of the proposed deictic gesture-based interface to facilitate intuitive human-robot interaction and precise operation of construction robots, particularly in situations where as-planned building models are not readily available. </p> </div> </div> </div> </li> <li> <div class="row"> <div id="Yoon2024evaluating" class="col-sm-10"> <div class="title">Evaluating Viewpoint Control Techniques in Virtual Reality Interface for Teleoperating Construction Welding Robots</div> <div class="author"> <em>Sungboo Yoon</em>, Seungmin Shin, Sanghyun Lee, Moonseo Park, and Changbum R Ahn </div> <div class="periodical"> <em>In Proceedings of the 31st International Workshop on Intelligent Computing in Engineering</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://3dgeoinfoeg-ice.webs.uvigo.es/proceedings" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2407_EG-ICE.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/2404_ICRA.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Effective visual feedback is critical for successful teleoperation of construction robots in remote locations. Recently, virtual reality (VR) has been leveraged to enhance teleoperation interfaces, providing enriched visualization and interaction capabilities. However, despite the critical role of viewpoints in 3D exploration and manipulation, the choice between coupled and decoupled viewpoints—where the user’s camera is tethered or independent from the end-effector’s position—in VR teleoperation interfaces for construction robots remains unclear. In this study, we explore the effect of viewpoint control technique in VR (coupled vs. decoupled viewpoint) on task performance, perceived workload, and perceived usability in the context of teleoperating welding robots in construction. Our comparative study with 10 participants demonstrated that under the coupled condition, participants completed the welding tasks significantly faster and with greater consistency across different locations when compared to the decoupled condition. These findings offer design implications for the integration of viewpoint control techniques in the development of visual interfaces for robotic teleoperation in construction.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Lee2024Interpreting" class="col-sm-10"> <div class="title">Interpreting Spatial Instructions for Effective Human-Robot Communication in Construction Environments</div> <div class="author"> Chaeeun Lee, <em>Sungboo Yoon</em>, Moonseo Park, and Changbum R. Ahn </div> <div class="periodical"> <em>Korean Society of Automation and Robotics in Construction in Journal of Construction Automation and Robotics</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.55785/jcar.3.3.6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.scilit.com/publications/cb04c7c3a54984f41ea61a90b3eccd29" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2408_KSARC.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>When utilizing robots to perform tasks in construction sites, it is important to ensure efficient communication between humans and robots based on spatial instructions and the intuitive decision-making of the operator, in order to adapt flexibly to changes in the environment or alterations in plans. However, it is challenging to accurately conveying the intent of the operator to the robot in complex construction environments. This becomes particularly difficult when dealing with tasks involving objects with curvature or geometric patterns, which increase the complexity of robot trajectory generation. Therefore, this study proposes a method to estimate the robot trajectory from spatial instruction that include deviation, specifically for targets with various forms. It focuses on interpreting spatial instructions in circular, rectangular, and linear shapes commonly used in construction, and the proposed approach involves two steps: classifying the trajectory shapes from spatial instruction and fitting the shapes through regression analysis. The accuracy of this method was evaluated in a scenario where 8 participants collaborated with a robot to cut ceiling-mounted components using spatial instructions provided through a laser pointer. As a result, the proposed method achieved an average Root Mean Squared Error of 2.398mm, compared to 7.274mm for the conventional B-Spline method. These results suggest that the regression analysis-based approach to interpreting spatial instructions has the potential to improve safety and productivity in construction tasks that rely on design plans and layouts.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="kang2024analysis" class="col-sm-10"> <div class="title">Analysis of Factors Affecting the Acceptance of Smart Personal Protective Equipment (Smart-PPE) by Construction Workers Based on the Technology Acceptance Model (TAM) Framework</div> <div class="author"> Dongwoo Kang, <em>Sungboo Yoon</em>, Moonseo Park, and Changbum R Ahn </div> <div class="periodical"> <em>Journal of the Korea Institute of Construction Safety</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://koreascience.kr/article/JAKO202407861206919.page#full-text-html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2406_JCS.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This study investigates the acceptance of smart personal protective equipment(S-PPE) by construction workers using the Technology Acceptance Model(TAM). Despite governmental policy support, the actual adoption rate remains low. This research aims to identify key variables influencing acceptance, focusing on perceived usefulness(PU) and perceived ease of use(PEOU). Testbeds were implemented across various construction sites, and data were collected and analyzed through semi-structured in-depth interviews based on TAM. The results indicate that PEOU has a greater impact on the acceptance of S-PPE than PU. Therefore, to accelerate the adoption of S-PPE in construction sites, it is essential to prioritize improving PEOU in technology development and application.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Yoon2024Taxonomy" class="col-sm-10"> <div class="title">A Taxonomy of Extended Reality for Human-Robot Interaction in Construction Based on a Systematic Literature Review</div> <div class="author"> <em>Sungboo Yoon</em>, Chaeeun Lee, SangHyun Lee, Moonseo Park, and Changbum R. Ahn </div> <div class="periodical"> <em>In Proceedings of the Creative Construction Conference 2024</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3311/CCC2024-039" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://repozitorium.omikk.bme.hu/items/986b3bab-a444-42ca-b444-552d5cb98594" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2406_CCC.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Extended Reality (XR) applications for Human-Robot Interaction (HRI) in construction, while still in a developmental stage, are rapidly gaining much attention for their ability to enhance communication with robots. However, this area of research often consists of individual explorations, leading to a lack of uniformity in terminology and interaction design techniques. Our study addresses this issue by proposing a comprehensive taxonomy for XR-HRI in construction. We conducted an exhaustive literature review of 51 papers in the construction domain to synthesize the state of the field. Our findings led to the construction of a novel taxonomy comprising three primary design spaces: (1) interface, (2) interaction, and (3) context. Our work contributes significantly to the field by providing a foundational framework that supports researchers and practitioners in the systematic development, standardization, and evaluation of XR-HRI designs in construction.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div id="yoon2023effects" class="col-sm-10"> <div class="title">Effects of Spatial Characteristics on the Human–Robot Communication Using Deictic Gesture in Construction</div> <div class="author"> <em>Sungboo Yoon</em>, YeSeul Kim, Moonseo Park, and Changbum R Ahn </div> <div class="periodical"> <em>Journal of Construction Engineering and Management</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1061/JCEMD4.COENG-12997" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://ascelibrary.org/doi/abs/10.1061/JCEMD4.COENG-12997" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2307_JCEM.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Construction robots are expected to frequently communicate in situ improvisations with human workers to adapt and change their workflow and methods. One way to achieve this is through deictic gestures that are one of the most effective forms of human–robot interaction (HRI) in delivering spatial information. Nevertheless, the limited coverage of deictic gestures in large-scale environments poses some challenges for both humans and robots in leveraging such techniques for HRI in construction. To identify the feasibility of deictic gestures in the construction domain and find applicable solutions for improving performance, this study aims to extend current knowledge on the performance in communicating positional information using deictic gestures by investigating the effects of spatial characteristics on spatial referencing, focusing on the target configuration, target distance, and relative position of human and robot. We observed that the recognition and estimation of deictic gestures were affected by the target plane, target position, and the target layout and that the robot performance was significantly reduced as the distance between the human and robot increased. The findings of this study demonstrate the challenges in spatial referencing within a large-scale environment and highlight the need for bidirectional communication in HRI.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div id="Heo2022Measuring" class="col-sm-10"> <div class="title">Measuring the Impact of Supply Network Topology on the Material Delivery Robustness in Construction Projects</div> <div class="author"> Chan Heo, Changbum Ahn, <em>Sungboo Yoon</em>, Minhyeok Jung, and Moonseo Park </div> <div class="periodical"> <em></em> 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div id="yoon2021challenges" class="col-sm-10"> <div class="title">Challenges in deictic gesture-based spatial referencing for human-robot interaction in construction</div> <div class="author"> <em>Sungboo Yoon</em>, Yeseul Kim, Changbum R Ahn, and Moonseo Park </div> <div class="periodical"> <em>In ISARC. Proceedings of the International Symposium on Automation and Robotics in Construction</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.iaarc.org/publications/2021_proceedings_of_the_38th_isarc/challenges_in_deictic_gesture_based_spatial_referencing_for_human_robot_interaction_in_construction.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2111_ISARC.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>As robots are envisioned to be deployed in construction job sites to work with humans, there is an increasing need for developing intuitive and natural communication between robots and humans. In particular, spatial information exchange is critical to navigating or delegating tasks to collaborative robots. However, such deictic gestures are inherently imprecise and ambiguous. Thus, it is challenging for robots to reason about the exact region of interest, especially in a cluttered large-scale construction environment. To address this limitation, this study evaluates the performance of spatial information exchange through the experiments based on pointing targets on the wall and ceiling, which are the most common workspaces in construction. We observed that the current deictic gesture-based method can estimate the pointed position on the wall and ceiling with a mean distance error of 0.767m, while the error tends to increase by 0.715m in the ceiling and 0.115m in the side panels. Our experimental results indicate that the deictic gesture-based method has some challenges in ceiling and side panel conditions, while the overall panel recognition shows acceptable performance. The findings of this study will help novice construction workers naturally and effectively communicate with robots by delivering spatial information on specific objects or regions in the shared workspace.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yoon2021multi" class="col-sm-10"> <div class="title">Multi-objective Optimization Model for Tower Crane Layout Planning in Modular Construction</div> <div class="author"> <em>Sungboo Yoon</em>, Moonseo Park, Minhyuk Jung, Hosang Hyun, and Suho Ahn </div> <div class="periodical"> <em>Korean Journal of Construction Engineering and Management</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1061/JCEMD4.COENG-12997" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://koreascience.kr/article/JAKO202109554085158.page" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2101_KJCEM.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>With an increasing trend toward high-rise modular construction, the simultaneous use of tower cranes at a modular construction site has recently been observed. Tower crane layout planning (TCLP) has a significant effect on cost, duration, safety and productivity of a project. In a modular construction project, particularly, poor decision about the layout of tower cranes is likely to have negative effects like additional employment of cranes and redesign, which will lead to additional costs and possible delays. It is, therefore, crucial to conduct thorough inspection of field conditions, lifting materials, tower crane capacity to make decisions on the layout of tower cranes. However, several challenges exist in planning for a multi-crane construction site in terms of safety and collaboration, which makes planning with experience and intuition complicated. This paper suggests a multi-objective optimization model for selection of the number of tower cranes, their models and locations, which minimizes cost and conflict. The proposed model contributes to the body of knowledge by showing the feasibility of using multi-objective optimization for TCLP decision-making process with consideration of trade-offs between cost and conflict.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Sungboo Yoon. Building 39 (Room 425), 1 Gwanak-ro, Gwanak-gu, Seoul, South Korea 08826 Last updated: September 15, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>